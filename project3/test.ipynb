{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Uncomment below to execute in workspace\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import etl\n",
    "from resource_reader import *\n",
    "from sql_queries import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# 1. Load configuration paramater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs loaded!\n"
     ]
    }
   ],
   "source": [
    "config_file_name = 'dwh.yml'\n",
    "rr = ResourceReader()\n",
    "config = rr.config_reader(config_file_name)\n",
    "\n",
    "CLUSTER_TYPE = config['CLUSTER']['TYPE']\n",
    "NODE_NUMBER = int(config['CLUSTER']['NUMBER_OF_NODE'])\n",
    "NODE_TYPE = config['CLUSTER']['NODE_TYPE']\n",
    "CLUSTER_IDENTIFIER = config['CLUSTER']['CLUSTER_IDENTIFIER']\n",
    "\n",
    "DB_NAME = config['CLUSTER']['DB_NAME']\n",
    "DB_USERNAME = config['CLUSTER']['DB_USER']\n",
    "DB_PASSWORD = config['CLUSTER']['DB_PASSWORD']\n",
    "PORT = config['CLUSTER']['DB_PORT']\n",
    "\n",
    "ACCESS_KEY_ID = config['AWS']['ACCESS_KEY_ID']\n",
    "SECRET_ACCESS_KEY = config['AWS']['SECRET_ACCESS_KEY']\n",
    "IAM_ROLE_NAME = config['IAM_ROLE']['NAME']\n",
    "\n",
    "WAIT = int(config['CLUSTER']['DEFAULT_WAIT'])\n",
    "TIMEOUT = int(config['CLUSTER']['TIMEOUT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# 2. Init AWS services (IAM, S3, EC2, and Redshift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Establish IAM, S3, EC2, and Redshift services\n"
     ]
    }
   ],
   "source": [
    "rr.setup_aws_services(ACCESS_KEY_ID, SECRET_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 2.1 Create IAM Role\n",
    "### Specify <font color=red>IAM Role Name</font> in config parameter: `configs['IAM_ROLE']['NAME']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role_name_test_for_AmazonS3ReadOnlyAccess3\n",
      "IAM role ARN: [arn:aws:iam::587843818199:role/role_name_test_for_AmazonS3ReadOnlyAccess3]\n",
      "Attach role policy successful\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '7c64bed2-c9fb-42b1-9515-ba570e95aced',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7c64bed2-c9fb-42b1-9515-ba570e95aced',\n",
       "   'content-type': 'text/xml',\n",
       "   'content-length': '212',\n",
       "   'date': 'Wed, 24 Aug 2022 12:04:11 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment to create the cluster\n",
    "# Create IAM Role if it is unavailalbe\n",
    "arn_test = rr.get_iam_role_arn(config)\n",
    "if 'arn:aws:iam' not in arn_test:\n",
    "    rr.create_iam_role(config)\n",
    "# Apply default requirement policy `AmazonS3ReadOnlyAccess` arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess\n",
    "rr.apply_policy(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 2.2 Create Redshift Cluster\n",
    "### a. Specify <font color=red>CLUSTER NAME</font> in config parameter: `configs['CLUSTER']['CLUSTER_IDENTIFIER']`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] fetching retry...\n",
      "Cluster state:  available\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Uncomment to create new Redshift Cluster\n",
    "# response = rr.create_redshift_cluster(config)\n",
    "#print(response)\n",
    "# Bear with me while cluster is ready\n",
    "# Cluster's state: [paused, resuming, Modifying... available]\n",
    "rr.wait_until_cluster_status(CLUSTER_IDENTIFIER, 'available', WAIT, TIMEOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### b. Get information of created Redshift Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster name:  redshift-cluster-useast\n",
      "Cluster endpoint:  redshift-cluster-useast.cw5jqkebdvqv.us-east-1.redshift.amazonaws.com\n",
      "Cluster port:  5439\n",
      "DB name:  dev\n",
      "Username:  student\n"
     ]
    }
   ],
   "source": [
    "# paused, resuming, available\n",
    "status = 'available'\n",
    "specs = rr.get_cluster_description(CLUSTER_IDENTIFIER)\n",
    "CLUSTER_NAME = specs['ClusterIdentifier']\n",
    "USERNAME = specs['MasterUsername']\n",
    "DB_NAME = specs['DBName']\n",
    "CLUSTER_ENDPOINT_URL = specs['Endpoint']['Address']\n",
    "CLUSTER_ENDPOINT_PORT = specs['Endpoint']['Port']\n",
    "ROLE_ARN = specs['IamRoles'][0]['IamRoleArn']\n",
    "print('Cluster name: ', CLUSTER_NAME)\n",
    "print('Cluster endpoint: ', CLUSTER_ENDPOINT_URL)\n",
    "print('Cluster port: ', CLUSTER_ENDPOINT_PORT)\n",
    "print('DB name: ', DB_NAME)\n",
    "print('Username: ', USERNAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### c. Configure incoming traffic to the cluster endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred (InvalidPermission.Duplicate) when calling the AuthorizeSecurityGroupIngress operation: the specified rule \"peer: 0.0.0.0/0, TCP, from port: 5439, to port: 5439, ALLOW\" already exists\n"
     ]
    }
   ],
   "source": [
    "rr.configure_incoming_tcp_traffic(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 2.3 Check Out Resources in AWS S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Load a few sample data sets to reduce consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'song_data/A/A/A/TRAAAAW128F429D538.json'\n",
      "'song_data/A/A/A/TRAAABD128F429CF47.json'\n",
      "'song_data/A/A/A/TRAAADZ128F9348C2E.json'\n",
      "'song_data/A/A/A/TRAAAEF128F4273421.json'\n",
      "'song_data/A/A/A/TRAAAFD128F92F423A.json'\n",
      "SONG DATA COUNT:  71\n",
      "'log_data/2018/11/2018-11-01-events.json'\n",
      "'log_data/2018/11/2018-11-02-events.json'\n",
      "'log_data/2018/11/2018-11-03-events.json'\n",
      "'log_data/2018/11/2018-11-04-events.json'\n",
      "'log_data/2018/11/2018-11-05-events.json'\n",
      "LOG DATA COUNT:  30\n",
      "'log_json_path.json'\n",
      "{ 'jsonpaths': [ \"$['artist']\",\n",
      "                 \"$['auth']\",\n",
      "                 \"$['firstName']\",\n",
      "                 \"$['gender']\",\n",
      "                 \"$['itemInSession']\",\n",
      "                 \"$['lastName']\",\n",
      "                 \"$['length']\",\n",
      "                 \"$['level']\",\n",
      "                 \"$['location']\",\n",
      "                 \"$['method']\",\n",
      "                 \"$['page']\",\n",
      "                 \"$['registration']\",\n",
      "                 \"$['sessionId']\",\n",
      "                 \"$['song']\",\n",
      "                 \"$['status']\",\n",
      "                 \"$['ts']\",\n",
      "                 \"$['userAgent']\",\n",
      "                 \"$['userId']\"]}\n",
      "LOG JSON DATA COUNT:  1\n"
     ]
    }
   ],
   "source": [
    "rr.check_out_s3_resource(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 3. Init Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB connected!\n"
     ]
    }
   ],
   "source": [
    "conn_string = \"host={} dbname={} user={} password={} port={}\".format(\n",
    "        CLUSTER_ENDPOINT_URL, DB_NAME, USERNAME,\n",
    "        config['CLUSTER']['DB_PASSWORD'], CLUSTER_ENDPOINT_PORT)\n",
    "\n",
    "conn = psycopg2.connect(conn_string)\n",
    "cur = conn.cursor()\n",
    "print('DB connected!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 4. Migrate staging data from S3 to staging tables on created Redshift Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.1 Migrate event logging, song data to staging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Migrate staging data. Query [\n",
      "    COPY staging_events\n",
      "    FROM 's3://myeyesbucket.test/log_data'\n",
      "    CREDENTIALS 'aws_iam_role=arn:aws:iam::587843818199:role/role_name_test_for_AmazonS3ReadOnlyAccess3'\n",
      "    FORMAT AS JSON 's3://myeyesbucket.test/log_json_path.json'\n",
      "    STATUPDATE ON\n",
      "    REGION 'us-east-1';\n",
      "]\n",
      "Migrate staging data. Query [\n",
      "    COPY staging_songs FROM 's3://myeyesbucket.test/song_data/'\n",
      "    CREDENTIALS 'aws_iam_role=arn:aws:iam::587843818199:role/role_name_test_for_AmazonS3ReadOnlyAccess3'\n",
      "    FORMAT AS JSON 'auto'\n",
      "    ACCEPTINVCHARS AS '^'\n",
      "    STATUPDATE ON\n",
      "    REGION 'us-east-1';\n",
      "]\n",
      "Staging data migrated. DONE!\n"
     ]
    }
   ],
   "source": [
    "etl.migrate_staging_table_data(cur, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.2 Populate analytical data into relevant tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert data. Query [\n",
      "    INSERT INTO songplays (start_time, user_id, level, song_id, artist_id,  session_id, location, user_agent)                \n",
      "    SELECT DISTINCT \n",
      "        TIMESTAMP 'epoch' + se.ts/1000 * INTERVAL '1 second', userId, \n",
      "        level, song_id, artist_id, sessionId, location, userAgent\n",
      "    FROM staging_songs ss \n",
      "    JOIN staging_events se ON (ss.artist_name = se.artist AND ss.title = se.song)\n",
      "    WHERE se.page = 'NextSong';\n",
      "]\n",
      "Insert data. Query [\n",
      "    INSERT INTO users (user_id, first_name, last_name, gender, level)\n",
      "    SELECT DISTINCT userId, firstName, lastName, gender, level \n",
      "    FROM staging_events\n",
      "    WHERE page ='NextSong' AND userId IS NOT NULL;\n",
      "]\n",
      "Insert data. Query [\n",
      "    INSERT INTO songs (song_id, song_title, artist_id, year, duration)\n",
      "    SELECT DISTINCT song_id, title, artist_id, year, duration\n",
      "    FROM staging_songs\n",
      "    WHERE song_id IS NOT NULL;\n",
      "]\n",
      "Insert data. Query [\n",
      "    INSERT INTO artists (artist_id, artist_name, artist_location, artist_latitude, artist_longitude)\n",
      "    SELECT DISTINCT artist_id, artist_name, artist_location, artist_latitude, artist_longitude\n",
      "    FROM staging_songs;\n",
      "]\n",
      "Insert data. Query [\n",
      "    INSERT INTO time (start_time, hour, year, month, day, week, weekday)\n",
      "    SELECT DISTINCT start_time, EXTRACT(HOUR FROM start_time) AS hour,\n",
      "    EXTRACT(YEAR FROM start_time) AS year, EXTRACT(MONTH FROM start_time) AS month,\n",
      "    EXTRACT(DAY FROM start_time) AS day, EXTRACT(WEEK FROM start_time) AS week,\n",
      "    EXTRACT(DOW FROM start_time) as weekday\n",
      "    FROM songplays;\n",
      "]\n",
      "Insert data completed. DONE!\n"
     ]
    }
   ],
   "source": [
    "etl.insert_analytical_table_data(cur, conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 4.3 Inquiry migrated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136952\n"
     ]
    }
   ],
   "source": [
    "cur.execute('SELECT COUNT(*) FROM staging_events')\n",
    "print(cur.fetchall()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT COUNT(*) FROM staging_songs'\n",
    "cur.execute(query)\n",
    "print(cur.fetchall()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2080\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT COUNT(*) FROM users'\n",
    "cur.execute(query)\n",
    "print(cur.fetchall()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "451\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT COUNT(*) FROM artists'\n",
    "cur.execute(query)\n",
    "print(cur.fetchall()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT COUNT(*) FROM time'\n",
    "cur.execute(query)\n",
    "print(cur.fetchall()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "query = 'SELECT COUNT(*) FROM songplays'\n",
    "cur.execute(query)\n",
    "print(cur.fetchall()[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## 5. Tear down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Uncomment to delete created Redshift Cluster\n",
    "# rr.tear_down_redshift_cluster(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
